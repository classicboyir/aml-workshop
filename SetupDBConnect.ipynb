{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Compute Instance as Development Environment for Databricks\n",
        "\n",
        "In this section, we'll go through the steps needed to setup DB Connect to connect to Databricks from Compute Instance:\n",
        "\n",
        "First step is to run the following commands in the terminal section."
      ],
      "metadata": {
        "gather": {
          "logged": 1621480620629
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "conda create -n dbconnect_env_py37 python=3.7\n",
        "conda activate dbconnect_env_py37\n",
        "conda install pip\n",
        "conda install ipykernel\n",
        "python -m ipykernel install --user --name dbconnect_env_py37 --display-name \"DBConnect Env 3.7\"\n",
        "\n",
        "pip install databricks-connect==7.3.5\n",
        "databricks-connect configure\n",
        "```"
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, provide the following information in the setup:\n",
        "1. Host: https://adb-5555555555555555.19.azuredatabricks.net/\n",
        "1. Token: From Databricks Workspace User Settings [Generate a personal access token](https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token)\n",
        "1. Cluster ID: from Databricks Workspace Cluster --> Advanced Settings --> Tags (Enter it manually) - `cluster databricks runtime should be 7.3.5`\n",
        "1. Org ID: the part in URL after .net/?o= https://adb-5555555555555555.19.azuredatabricks.net/?o=123...\n",
        "1. Keep the port as-is\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the set up is completed, open a new notebook and select `DBConnect 3.7`, **start** the databricks cluster and run the following command.\n",
        "\n",
        "![Hyper parameter tuning](assets/runtime.jpg)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -P ./data https://dprepdata.blob.core.windows.net/demo/Titanic.csv"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-05-20 15:44:37--  https://dprepdata.blob.core.windows.net/demo/Titanic.csv\r\n",
            "Resolving dprepdata.blob.core.windows.net (dprepdata.blob.core.windows.net)... 52.239.160.170\r\n",
            "Connecting to dprepdata.blob.core.windows.net (dprepdata.blob.core.windows.net)|52.239.160.170|:443... connected.\r\n",
            "HTTP request sent, awaiting response... 200 OK\r\n",
            "Length: 61194 (60K) [text/csv]\r\n",
            "Saving to: ‘./data/Titanic.csv.2’\r\n",
            "\r\n",
            "\rTitanic.csv.2         0%[                    ]       0  --.-KB/s               \r",
            "Titanic.csv.2       100%[===================>]  59.76K  --.-KB/s    in 0.02s   \r\n",
            "\r\n",
            "2021-05-20 15:44:38 (3.25 MB/s) - ‘./data/Titanic.csv.2’ saved [61194/61194]\r\n",
            "\r\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1623876524670
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfGridAVG = spark.read.format('delta').load('/mnt/delta/analytics/grid_growth/coarse_table_temp')"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1623876547272
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfGridAVG.count()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "126841198"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1623869737925
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "dbconnect_env_py37"
    },
    "kernelspec": {
      "name": "dbconnect_env_py37",
      "language": "python",
      "display_name": "DBConnect Env 3.7"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}