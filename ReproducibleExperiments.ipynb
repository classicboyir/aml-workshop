{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Reproducible Experiments\n",
        "In this notebook, the aim is to show you how to build a reproducible experiment within Azure. In this example, we're going to train a MNIST classifier using Tensorflow and show you how you can follow some practices to make this work reproducible. Later you can use this framework and apply it to other ML problems.\n",
        "\n",
        "The reason we chose MNIST is to pick a very simple example as the main focus is to build a reproducible experiment and not to learn a new algorithm or to build a complex model.\n",
        "\n",
        "By the end of this repo, you'll be able to build reproducible experiments based on the below image.\n",
        "![Reproducible Experiments for Machine Learning](assets/reproducible_experiment_azure_machine_learning.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we download the MNIST sample files from Yann Lecun website to our development environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628705372712
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "\n",
        "os.makedirs('./data/mnist', exist_ok=True)\n",
        "\n",
        "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz', filename = './data/mnist/train-images.gz')\n",
        "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz', filename = './data/mnist/train-labels.gz')\n",
        "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz', filename = './data/mnist/test-images.gz')\n",
        "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz', filename = './data/mnist/test-labels.gz')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import a bunch of packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261581906
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from azureml.widgets import RunDetails\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the AzureML SDK package to be able to communicate with Azure ML Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637777543989
        }
      },
      "outputs": [],
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "from azureml.core import Experiment\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate an object from Workspace class. the Workspace object will point to the created Workspace we created through the portal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268332634
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628705380877
        }
      },
      "outputs": [],
      "source": [
        "# # Run this section if you're running this from outside of Compute Instance\n",
        "# # import the Workspace class and check the azureml SDK version\n",
        "# # exist_ok checks if workspace exists or not.\n",
        "\n",
        "# from azureml.core import Workspace\n",
        "# \n",
        "\n",
        "# # Your subscription ID will be different replace the stirng with yours\n",
        "# subscription_id = \"your_subscription_id\"\n",
        "# resource_group = \"your_resource_group\"\n",
        "# workspace_name = \"your_workspace_name\"\n",
        "# workspace_region = \"your_workspace_region\"\n",
        "\n",
        "# ws = Workspace(workspace_name = workspace_name,\n",
        "#                subscription_id = subscription_id,\n",
        "#                resource_group = resource_group)\n",
        "# \n",
        "# # persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
        "# ws.write_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start a new Experiment\n",
        "\n",
        "In software engineering world we have a new feature to develop. In ML/Data Science world, we work on experiments.\n",
        "\n",
        "**Experiments** represent the collection of trials used to validate a user's hypothesis. We call each trial a run.\n",
        "\n",
        "Here we create a new experiment, we want to make sure everything related to this experiment is saved within the workspace and the lineage between each artifact is preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268360341
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "exp = Experiment(workspace=ws, name='at-rogers')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Track and Log Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have two ways to start a trail (Run). Interactively and through batch submite. The start_loggin() method is the interactive way of starting a trail. It returns a Run object that we can use to log important metrics or the trail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MLFlow is natively supported in Azure Machine Learning. For examples on MLFlow integration, take a look at [[using-mlflow](https://github.com/Azure/MachineLearningNotebooks/blob/bda592a236eaf2dbc54b394e1fa1b539e0297908/how-to-use-azureml/ml-frameworks/using-mlflow)]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The key metrics can a single value for the accuracy of an ML model, a list of values representing the distribution or the data or an image showing the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268433573
        }
      },
      "outputs": [],
      "source": [
        "# Initializing log tracking action by creating a Run object in the Experiment\n",
        "run = exp.start_logging()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268436586
        }
      },
      "outputs": [],
      "source": [
        "run.get_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here you see that the **azureml.git.repository_uri** is poinint to the remote repo and the **azureml.git.branch** property is poining to the active branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268439607
        }
      },
      "outputs": [],
      "source": [
        "run.get_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637777566356
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Run\n",
        "# run = Run(experiment=exp, run_id='ca910c28-d9fe-4c94-bc55-6ed76a015f82')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1637268450081
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run.get_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RunDetails** class helps you visualize the active run object. It creates a network connection with Azure ML Worspace to collect everything happening during the run. It gets updated every 15 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103042722
        }
      },
      "outputs": [],
      "source": [
        "RunDetails(run).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**logs** function will log a single-valued or multi-valued metric under the current RUN. There are several types of logs, metric, table, row, image, etc.\n",
        "\n",
        "Every time you add a new metric, check the widget above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268450925
        }
      },
      "outputs": [],
      "source": [
        "run.log('metric_1', 1.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268452097
        }
      },
      "outputs": [],
      "source": [
        "run.log('metric_1', 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268453377
        }
      },
      "outputs": [],
      "source": [
        "run.log('metric_1', 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268454606
        }
      },
      "outputs": [],
      "source": [
        "run.log('metric_1', 4.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268457706
        }
      },
      "outputs": [],
      "source": [
        "run.log('metric_1', 4.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268458684
        }
      },
      "outputs": [],
      "source": [
        "run.log('metric_1', 2.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268462683
        }
      },
      "outputs": [],
      "source": [
        "run.log('hossein', 6.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103350089
        }
      },
      "outputs": [],
      "source": [
        "# Log some metrics about the input dataset:\n",
        "\n",
        "from utils import load_data\n",
        "\n",
        "# Unzipping the input dataset and conver the data points into Numpy arrays\n",
        "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
        "X_train = load_data('./data/mnist/train-images.gz', False) / 255.0\n",
        "y_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n",
        "\n",
        "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
        "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103363818
        }
      },
      "outputs": [],
      "source": [
        "# Record how the input dataset looks like\n",
        "count = 0\n",
        "sample_size = 30\n",
        "plt.figure(figsize = (16, 6))\n",
        "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
        "    count = count + 1\n",
        "    plt.subplot(1, sample_size, count)\n",
        "    plt.axhline('')\n",
        "    plt.axvline('')\n",
        "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
        "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
        "    \n",
        "run.log_image(name='{}-samples-of-input-dataset'.format(sample_size), plot=plt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103365938
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dist = pd.DataFrame(data=y_train, columns=['test_values'])['test_values'].value_counts()\n",
        "\n",
        "run.log_table('digit_dist', {\"count\":list(dist.values), \"digits\":list(dist.index)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103455697
        }
      },
      "outputs": [],
      "source": [
        "run.log('train_dataset_size', X_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103463290
        }
      },
      "outputs": [],
      "source": [
        "run.log('test_dataset_size', X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the collected metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103524098
        }
      },
      "outputs": [],
      "source": [
        "print('metric_1: ', run.get_metrics('metric_1'))\n",
        "run.get_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268569910
        }
      },
      "outputs": [],
      "source": [
        "run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Track local model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103815116
        }
      },
      "outputs": [],
      "source": [
        "run = exp.start_logging()\n",
        "RunDetails(run).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103838413
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import glob\n",
        "\n",
        "from azureml.core import Run\n",
        "from utils import load_data\n",
        "from tensorflow.keras import Model, layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103862628
        }
      },
      "outputs": [],
      "source": [
        "training_set_size = X_train.shape[0]\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_h1 = 100\n",
        "n_h2 = 100\n",
        "n_outputs = 10\n",
        "learning_rate = 0.01\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "run.log_row('Hyper Parameters',\n",
        "        n_inputs=n_inputs,\n",
        "        n_h1=n_h1,\n",
        "        n_h2=n_h2,\n",
        "        n_outputs=n_outputs,\n",
        "        learning_rate=learning_rate,\n",
        "        n_epochs=n_epochs,\n",
        "        batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103875346
        }
      },
      "outputs": [],
      "source": [
        "# Create TF Model.\n",
        "class NeuralNet(Model):\n",
        "    # Set layers.\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # First hidden layer.\n",
        "        self.h1 = layers.Dense(n_h1, activation=tf.nn.relu)\n",
        "        # Second hidden layer.\n",
        "        self.h2 = layers.Dense(n_h2, activation=tf.nn.relu)\n",
        "        self.out = layers.Dense(n_outputs)\n",
        "\n",
        "    # Set forward pass.\n",
        "    def call(self, x, is_training=False):\n",
        "        x = self.h1(x)\n",
        "        x = self.h2(x)\n",
        "        x = self.out(x)\n",
        "        if not is_training:\n",
        "            # Apply softmax when not training.\n",
        "            x = tf.nn.softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y, logits):\n",
        "    # Convert labels to int 64 for tf cross-entropy function.\n",
        "    y = tf.cast(y, tf.int64)\n",
        "    # Apply softmax to logits and compute cross-entropy.\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    # Average loss across the batch.\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "# Accuracy metric.\n",
        "def accuracy(y_pred, y_true):\n",
        "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
        "\n",
        "\n",
        "# Optimization process.\n",
        "def run_optimization(x, y):\n",
        "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
        "    with tf.GradientTape() as g:\n",
        "        # Forward pass.\n",
        "        logits = neural_net(x, is_training=True)\n",
        "        # Compute loss.\n",
        "        loss = cross_entropy_loss(y, logits)\n",
        "\n",
        "    # Variables to update, i.e. trainable variables.\n",
        "    trainable_variables = neural_net.trainable_variables\n",
        "\n",
        "    # Compute gradients.\n",
        "    gradients = g.gradient(loss, trainable_variables)\n",
        "\n",
        "    # Update W and b following gradients.\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628103882957
        }
      },
      "outputs": [],
      "source": [
        "# Build neural network model.\n",
        "neural_net = NeuralNet()\n",
        "\n",
        "# Stochastic gradient descent optimizer.\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628104206830
        }
      },
      "outputs": [],
      "source": [
        "start_time = time.perf_counter()\n",
        "for epoch in range(0, n_epochs):\n",
        "\n",
        "    # randomly shuffle training set\n",
        "    indices = np.random.permutation(training_set_size)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    # batch index\n",
        "    b_start = 0\n",
        "    b_end = b_start + batch_size\n",
        "    for _ in range(training_set_size // batch_size):\n",
        "        # get a batch\n",
        "        X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
        "\n",
        "        # update batch index for the next batch\n",
        "        b_start = b_start + batch_size\n",
        "        b_end = min(b_start + batch_size, training_set_size)\n",
        "\n",
        "        # train\n",
        "        run_optimization(X_batch, y_batch)\n",
        "\n",
        "    # evaluate training set\n",
        "    pred = neural_net(X_batch, is_training=False)\n",
        "    acc_train = accuracy(pred, y_batch)\n",
        "\n",
        "    # evaluate validation set\n",
        "    pred = neural_net(X_test, is_training=False)\n",
        "    acc_val = accuracy(pred, y_test)\n",
        "\n",
        "    # log accuracies\n",
        "    run.log('training_acc', np.float(acc_train))\n",
        "    run.log('validation_acc', np.float(acc_val))\n",
        "    print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
        "\n",
        "    # Save checkpoints in the \"./outputs\" folder so that they are automatically uploaded into run history.\n",
        "    checkpoint_dir = './outputs/'\n",
        "    checkpoint = tf.train.Checkpoint(model=neural_net, optimizer=optimizer)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        checkpoint.save(checkpoint_dir)\n",
        "    time.sleep(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628104358374
        }
      },
      "outputs": [],
      "source": [
        "run.log('final_acc', np.float(acc_val))\n",
        "os.makedirs('./outputs/model', exist_ok=True)\n",
        "\n",
        "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
        "# this is workaround for https://github.com/tensorflow/tensorflow/issues/33913 and will be fixed once we move to >tf2.1\n",
        "neural_net._set_inputs(X_train)\n",
        "tf.saved_model.save(neural_net, './outputs/model/')\n",
        "\n",
        "stop_time = time.perf_counter()\n",
        "training_time = (stop_time - start_time) * 1000\n",
        "print(\"Total time in milliseconds for training: {}\".format(str(training_time)))\n",
        "run.log('runtime ms', np.float(training_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628104361057
        }
      },
      "outputs": [],
      "source": [
        "run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2- Working with Models\n",
        "### 2-1 Register Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105143170
        }
      },
      "outputs": [],
      "source": [
        "run.register_model('MNIST_TF_rogers', model_path='outputs/model/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637777585066
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105408125
        }
      },
      "outputs": [],
      "source": [
        "model_list = Model.list(ws)\n",
        "[x for x in model_list if x.name == 'MNIST_TF_rogers']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-2 Retrieve registered models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105423501
        }
      },
      "outputs": [],
      "source": [
        "model = Model(ws, 'MNIST_TF_rogers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105430620
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs('downloaded_model_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105445648
        }
      },
      "outputs": [],
      "source": [
        "model.download('./downloaded_model_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105481811
        }
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('./downloaded_model_2/model')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628105490431
        }
      },
      "outputs": [],
      "source": [
        "y_hat = model.predict(X_test)\n",
        "acc_val = accuracy(y_hat, y_test)\n",
        "print(np.float(acc_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Datastore and Datasets\n",
        "\n",
        "As an important part of experiment reproducibility, you'd like to separate your dataset from the training code and the development environment. There are two major ways that you can achieve this. First you can use datastores to define the connection to an Azure data store such as Blob, SQL or Databricks table, then you leverage Datasets to access the actual files and version the reference to those file assets.\n",
        "\n",
        "### 2.1 Generate datastore references for the ML job\n",
        "\n",
        "Once an ML Workspace is created, a storage account is created with a default container (a logical container that works as a folder - this is refered to as bucket in AWS S3). The container is attached to the Workspace automatically as the default storage account. You can find it as **workspaceblobstore** under Datastores (https://ml.azure.com). This storage account can be used for test and development but should not be used in production scenarios. Because if you decide to delete the Workspace, the default storage account is also deleted which results in losing your data. So it's wiser to create a separate storage account and attach it to the ML Workspace.\n",
        "\n",
        "Here is how to access the default storage account:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628705442354
        }
      },
      "outputs": [],
      "source": [
        "# get_default_datastore returns the default datastore attached to the Workspace\n",
        "\n",
        "ds = ws.get_default_datastore()\n",
        "\n",
        "# Retrieve other registered datastores\n",
        "# ds = Datastore(ws, \"generalpurposeaccount\")\n",
        "\n",
        "ds.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637269252857
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Datastore\n",
        "\n",
        "ds = Datastore(ws, \"main_datastore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the name of the default datastore suggestion, it's a reference to a Blob storage. Blob storage is a general purpose data lake that can be used to store any type of binary, from image, to csv file. Here are other types of Azure Storage types that you can attach to the WorkSpace: (read [this](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data#supported-data-storage-service-types) more info)\n",
        "\n",
        "- Azure File Share\n",
        "- Azure Data Lake\n",
        "- Azure Data Lake Gen2\n",
        "- Azure SQL Database\n",
        "- Azure PostgreSQL\n",
        "- Databricks File System\n",
        "\n",
        "The following is an example of registering (attaching) a new Blob storage account:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from azureml.core.datastore import Datastore\n",
        "# blob_datastore = Datastore.register_azure_blob_container(\n",
        "#            workspace=ws,\n",
        "#            datastore_name='<datastore_name>',\n",
        "#            account_name='<account_name>', # Storage account name\n",
        "#            container_name='<container_name>', # Name of Azure blob container\n",
        "#            account_key='<account_key>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's upload the files that I downloaded to this storage for longer retention. This datastore can be referenced later in my training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637269273287
        }
      },
      "outputs": [],
      "source": [
        "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Datasets\n",
        "\n",
        "There are two different types of Datasets, TabularDataset and FileDataset. The Tabular can be used to access tabular like datasources, such as csv, SQL, Databricks, etc which FileDatasets can be used for binary datasets such as image, audio, etc. The following is a way to define a tabular dataset from an online source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datasets are references to a datasource (registered under datasources or available over internet). In other words, they don't keep the data, they are the reference definitiosn. You can register the defined Datasets under the Datasets section (accessible through ml.azure.com). Each time you register the dataset under the same name, you'll get a new version generated. Later you can access a particular version for the registered Dataset. As the dataset doesn't store your data, if you remove or change the data, the Dataset object doesn't help you roll back the change. Therefore, it's recommended to keep the data untouched. \n",
        "\n",
        "In order to keep the actual dataset, you can copy the data (using Azure Data Factory SDK or AzCopy CLI tool or [Azure SDK](https://github.com/Azure/azure-sdk-for-python/tree/master/sdk/storage/azure-storage-blob)) to clone the data to a new blob location and keep the under a new Dataset object.\n",
        "\n",
        "In the following cells, I've generated two versions of the Dataset. Every version of the titanic_ds can have different reference structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now going back to our MNIST problem, let's create a Dataset object from the file uploaded to the default Datastore and register the Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637269482828
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "datastore_paths = [(ds, 'mnist/test-images.gz'),\n",
        " (ds, 'mnist/test-labels.gz'),\n",
        " (ds, 'mnist/train-images.gz'),\n",
        " (ds, 'mnist/train-labels.gz')]\n",
        "\n",
        "mnist_dataset = Dataset.File.from_files(datastore_paths)\n",
        "mnist_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637269356109
        }
      },
      "outputs": [],
      "source": [
        "mnist_dataset.register(workspace = ws,\n",
        "                                 name = 'mnist_dataset',\n",
        "                                 description = 'MNIST input dataset',\n",
        "                                 create_new_version = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Work with Tabular Files\n",
        "\n",
        "Tabular files can be csv and parquet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! wget -P ./data https://dprepdata.blob.core.windows.net/demo/Titanic.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pdf_titanic_csv = pd.read_csv('./data/Titanic.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds.upload_files(['./data/Titanic.csv'], target_path='titanic', overwrite=True, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic_dataset = Dataset.Tabular.from_delimited_files(path = [(ds, 'titanic/Titanic.csv')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_titanic = titanic_dataset.to_pandas_dataframe()\n",
        "pdf_titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic_dataset = titanic_dataset.register(ws, name='Titanic Dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At any time you can retrive a registered Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic_ds_v1 = Dataset.get_by_name(workspace = ws,\n",
        "                                 name = 'Titanic Dataset')\n",
        "\n",
        "titanic_ds_v1.take(3).to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's register it within the Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You define a new Dataset under **mnist_dataset** by accessing the actual file over the internet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637269710379
        }
      },
      "outputs": [],
      "source": [
        "web_paths = [\n",
        "            'https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
        "            'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
        "            'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
        "            'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz'\n",
        "            ]\n",
        "\n",
        "mnist_dataset_web = Dataset.File.from_files(path = web_paths)\n",
        "\n",
        "mnist_dataset_web.register(workspace = ws,\n",
        "                           name = 'mnist_dataset_web',\n",
        "                           description = 'MNIST input dataset from web')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637269738832
        }
      },
      "outputs": [],
      "source": [
        "mnist_dataset_v1 = Dataset.get_by_name(workspace = ws,\n",
        "                                 name = 'mnist_dataset_web')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1628705494541
        }
      },
      "outputs": [],
      "source": [
        "# get the list of registered files back\n",
        "mnist_dataset_v1.to_path()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.1 Training on Remote Compute\n",
        "\n",
        "In this section, we'd like to improve the previous local training experience by packaging the run in a more managed way so that the environment is also tracked and versioned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637778590136
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# choose a name for your cluster\n",
        "cluster_name = \"k80-2-g-2-n\" # \"v100-1-g-1-n\"\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing compute target')\n",
        "except ComputeTargetException:\n",
        "    print('Compute target not found...')\n",
        "\n",
        "# use get_status() to get a detailed status for the current cluster. \n",
        "print(compute_target.get_status().serialize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compute_targets = ws.compute_targets\n",
        "for name, ct in compute_targets.items():\n",
        "    print(name, ct.type, ct.provisioning_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "envs = Environment.list(workspace=ws)\n",
        "\n",
        "for env in envs:\n",
        "    if env.startswith(\"AzureML\"):\n",
        "        print(\"Name\",env)\n",
        "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637270254982
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "tf_env = Environment.get(ws, name='AzureML-TensorFlow-2.0-GPU')\n",
        "tf_env "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637270429675
        }
      },
      "outputs": [],
      "source": [
        "script_folder = './project-folder'\n",
        "exp = Experiment(workspace=ws, name='tf-mnist-2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Providing the dataset as an input parameter\n",
        "\n",
        "One of the ways to make the training job flexible is to provide parameters instead of counting on hardcoded values. One of the important values is the data folder. As we've discussed the two ways of providing data to the ML Workspace (Datastores and Datasets) you can similarly provide both to your training job.\n",
        "\n",
        "The as_mount function connects the dataset or datastore to the target compute at the run time, makes it look like accessing the files as if an external storage is mounted to the remote compute. This is very efficient, as it doesn't bring un-accessed data into the remote compute.\n",
        "\n",
        "The other options is to call as_download function, but it requires the remote compute to fiest download the entire data (or the corresponding batch of data) to the local compute before running the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637270443039
        }
      },
      "outputs": [],
      "source": [
        "mnist_dataset = Dataset.get_by_name(workspace = ws, name = 'mnist_dataset_web')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637270747697
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "args = ['--data-folder', mnist_dataset.as_mount(),\n",
        "        '--batch-size', 64,\n",
        "        '--first-layer-neurons', 256,\n",
        "        '--second-layer-neurons', 128,\n",
        "        '--learning-rate', 0.01]\n",
        "\n",
        "src = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script='tf_mnist.py',\n",
        "                      arguments=args,\n",
        "                      compute_target=compute_target,\n",
        "                      environment=tf_env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637270753782
        }
      },
      "outputs": [],
      "source": [
        "run = exp.submit(src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1637270756851
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# \n",
        "run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Returning a run from history\n",
        "# from azureml.core import ScriptRun\n",
        "# run = ScriptRun(exp, 'tf-mnist-2_1621465912_275dda41')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RunDetails(run).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AML Environments\n",
        "\n",
        "Software dependency management is a common task for developers. You want to ensure that builds are reproducible without extensive manual software configuration. The Azure Machine Learning `Environment` class accounts for local development solutions such as pip and Conda and distributed cloud development through Docker capabilities.\n",
        "\n",
        "To learn more, check out: [Create & use software environments in Azure Machine Learning\n",
        "](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments)\n",
        "\n",
        "The Environment class is used to provide system configurations such as pip, and conda dependancies, base docker file or images and anything in between. You could use curated (already defined by Microsoft) environment or create a custom environment.\n",
        "\n",
        "AML has several curated environment that could be used as is. Most of relevant depandancies are already installed so minimal changes is needed. To find the predefined environment you can run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "envs = Environment.list(workspace=ws)\n",
        "\n",
        "for env in envs:\n",
        "    if env.startswith(\"AzureML\"):\n",
        "        print(f\"Name: [{env}]\")\n",
        "        # print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create your custom environment, you can start with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.environment import Environment\n",
        "my_custom_env = Environment(name=\"my_custom_environment\")\n",
        "\n",
        "# Or clone from a curated environment\n",
        "\n",
        "tf_env = Environment.get(ws, name='AzureML-TensorFlow-2.0-GPU')\n",
        "tf_env_custom = tf_env.clone('Custom_AzureML-TensorFlow-2.0-GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_env_custom.python.conda_dependencies.add_pip_package(\"seaborn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_env_custom.python.conda_dependencies.serialize_to_string()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally you can register and retrieve it later for other workloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_env_custom = tf_env_custom.register(workspace=ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "envs = Environment.list(workspace=ws)\n",
        "envs['Custom_AzureML-TensorFlow-2.0-GPU']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remote Job Flow\n",
        "\n",
        "![Remote Job Flow](assets/RemoteJobOrchestration.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register the model into the Model Registry\n",
        "\n",
        "The Run object can access to the output folder that is saved under the current Run within the experiment. By using the register_model function, you can register the model under the Model tab of the Workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.register_model(model_name='tf-dnn-mnist-single-run', model_path='outputs/model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Logging and Reproducibility\n",
        "\n",
        "When the script is submitted using an Estimator object, Azure ML deploys a GPU base Linux docker image on the remote compute and based on the pip_packages and conda_packages parameters you can reproduce the environment as you wish.\n",
        "\n",
        "Once the Run is submitted, the entire environment dependencies are logged and can be reproduced. Using the Run Details, you can explore the docker, conda and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "run.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training V2 - Hyperparameter tuning\n",
        "\n",
        "In most of our DS experiments, in order to get to the optimal model, you have to try several hyper parameters on your algorithm. This can become super slow and time consuming. Using HyperDrive namespace, you can provide several choices to be passed to your training script through the parameters. In the example below, I provided three choices for the batch-size, 5 choices for the first layer neurons, 4 choices for the second-layer-neurons and a random value from the continuous space of log uniform distribution for the learning-rate.\n",
        "\n",
        "Later by setting the max_total_runs as a parameter to the HyperDriveConfig, you can set the total number of tries. In the example below, it uses Random Sampling technique to find the next combination of hyperparameters to try. It will stop based on the BanditPolicy or if number of iterations reaches the max_total_runs.\n",
        "\n",
        "Here are the types of sampling the hyperparamer space:\n",
        "* Random sampling\n",
        "* Grid sampling\n",
        "* Bayesian sampling\n",
        "\n",
        "Link: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters\n",
        "\n",
        "In case of having several nodes (in our case we only have 1 node), you can submit several parallel execution on multiple nodes by setting the max_concurrent_runs. For example, if you have set max_total_runs = 100 and set max_concurrent_runs to 20. Assuming that you have 20 nodes of GPU cluster, then there will be 20 concurrent runs on all of the 20 nodes. Therefore, in theory you require 5 cycles to compelete the entire 100 experiments.\n",
        "\n",
        "\n",
        "![Hyper parameter tuning](assets/hyperparameter_tuning.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.train.hyperdrive import RandomParameterSampling, choice, loguniform\n",
        "\n",
        "ps = RandomParameterSampling(\n",
        "    {\n",
        "        '--batch-size': choice(32, 64, 128),\n",
        "        '--first-layer-neurons': choice(16, 64, 128, 256, 512),\n",
        "        '--second-layer-neurons': choice(16, 64, 256, 512),\n",
        "        '--learning-rate': loguniform(-6, -1)\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = ['--data-folder', mnist_dataset.as_mount()]\n",
        "\n",
        "src = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script='tf_mnist.py',\n",
        "                      arguments=args,\n",
        "                      compute_target=compute_target,\n",
        "                      environment=tf_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will define an early `termnination policy`. This will terminate poorly performing runs automatically, reducing wastage of resources and instead efficiently using these resources for exploring other parameter configurations. In this example, we will use the TruncationSelectionPolicy, truncating the bottom performing 25% runs. It states to check the job every 2 iterations. If the primary metric (defined later) falls in the bottom 25% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
        "policy = TruncationSelectionPolicy(evaluation_interval=2, truncation_percentage=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
        "htc = HyperDriveConfig(run_config=src, \n",
        "                       hyperparameter_sampling=ps, \n",
        "                       policy=policy, \n",
        "                       primary_metric_name='validation_acc', \n",
        "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
        "                       max_total_runs=4,\n",
        "                       max_concurrent_runs=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "htr = exp.submit(config=htc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Returning a Hyper Drive run from history\n",
        "# from azureml.train.hyperdrive import HyperDriveRun\n",
        "# htr = HyperDriveRun(exp, 'HD_2bb4dc41-d40d-4f40-adba-b599202756d1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RunDetails(htr).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "htr.wait_for_completion(show_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this run as you'll be having several models to collect. You can always get the **best** or collect the entire generated models. In the example below, the best model is retrieved and registered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_run = htr.get_best_run_by_primary_metric()\n",
        "print(best_run.get_file_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = best_run.register_model(model_name='tf-dnn-mnist-hyperp-tunning', model_path='outputs/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training V3 - AutoML\n",
        "\n",
        "Automated machine learning, also referred to as automated ML or AutoML, is the process of automating the time consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality. Automated ML in Azure Machine Learning is based on a breakthrough from our Microsoft Research division.\n",
        "\n",
        "To learn more, please visit: [What is automated machine learning (AutoML)?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's try to train model using Azure ML AutoML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.DataFrame(data=X_train)\n",
        "df_train['target'] = y_train\n",
        "\n",
        "df_train.to_csv('./data/mnist_train.csv')\n",
        "\n",
        "ds.upload_files(['./data/mnist_train.csv'], target_path='mnist_numpy', overwrite=True, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "mnist_dataset = Dataset.Tabular.from_delimited_files(path = [(ds, 'mnist_numpy/mnist_train.csv')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# choose a name for your cluster\n",
        "cluster_name = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing compute target')\n",
        "    print(compute_target.get_status().serialize())\n",
        "except ComputeTargetException:\n",
        "    print('Compute target not found...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.automl.core.featurization import FeaturizationConfig\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.interpret import ExplanationClient\n",
        "\n",
        "\n",
        "automl_settings = {\n",
        "    \"experiment_timeout_hours\" : 10,\n",
        "    \"enable_early_stopping\" : True,\n",
        "    \"iteration_timeout_minutes\": 5,\n",
        "    \"max_concurrent_iterations\": 4,\n",
        "    \"max_cores_per_iteration\": -1,\n",
        "    #\"n_cross_validations\": 2,\n",
        "    \"primary_metric\": 'AUC_weighted',\n",
        "    \"featurization\": 'auto',\n",
        "    \"verbosity\": logging.INFO,\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automl_errors.log',\n",
        "                             compute_target=compute_target,\n",
        "                             experiment_exit_score = 0.9984,\n",
        "                             blocked_models = ['KNN','LinearSVM'],\n",
        "                             enable_onnx_compatible_models=True,\n",
        "                             training_data = mnist_dataset,\n",
        "                             label_column_name = 'target',\n",
        "                             **automl_settings\n",
        "                            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment=Experiment(ws, 'automl_mnist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "remote_run = experiment.submit(automl_config, show_output = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(remote_run).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "remote_run.wait_for_completion(show_output=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register the best model based on the primary metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_run, fitted_model = remote_run.get_output()\n",
        "best_run.register_model(model_name = 'automl_mnist', model_path='outputs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261457531
        }
      },
      "outputs": [],
      "source": [
        "model = Model(ws, 'automl_mnist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268822012
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "import mlflow\n",
        "import mlflow.azureml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268859387
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268880955
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "experiment_name = \"at-rogers\"\n",
        "mlflow.set_experiment(experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261735598
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "experiment_name = \"pytorch-with-mlflow\"\n",
        "mlflow.set_experiment(experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637268970052
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run():\n",
        "    mlflow.log_metric(\"metric_mlflow\", 1.1)\n",
        "    mlflow.log_metric(\"metric_mlflow\", 3)\n",
        "    mlflow.log_metric(\"metric_mlflow\", 3.2)\n",
        "    mlflow.log_metric(\"metric_mlflow\", 2.2)\n",
        "    mlflow.log_metric(\"metric_mlflow\", 8.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261743849
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "env = Environment.get(workspace=ws, name=\"AzureML-PyTorch-1.4-GPU\").clone(\"mlflow-env\")\n",
        "\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-mlflow\")\n",
        "env.python.conda_dependencies.add_pip_package(\"Pillow==6.0.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261807739
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "src = ScriptRunConfig(source_directory=\"./mlflow\", script=\"train.py\")\n",
        "src.run_config.environment = env\n",
        "src.run_config.target = \"gpu-cluster\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261821900
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "exp = Experiment(ws, experiment_name)\n",
        "run = exp.submit(src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637261832019
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1637262645393
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run.register_model('MLFlow', model_path='model/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "experiment_name = \"keras-with-mlflow\"\n",
        "mlflow.set_experiment(experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "env = Environment.get(workspace=ws, name=\"AzureML-TensorFlow-2.1-GPU\").clone(\"mlflow-env\")\n",
        "\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-mlflow\")\n",
        "env.python.conda_dependencies.add_pip_package(\"keras==2.3.1\")\n",
        "env.python.conda_dependencies.add_pip_package(\"numpy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "src = ScriptRunConfig(source_directory=\"./mlflow\", script=\"train_autolog.py\")\n",
        "src.run_config.environment = env\n",
        "src.run_config.target = \"gpu-clu-custom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "exp = Experiment(ws, experiment_name)\n",
        "run = exp.submit(src)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
