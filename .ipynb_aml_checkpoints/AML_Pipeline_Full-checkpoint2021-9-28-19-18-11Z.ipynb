{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute, DatabricksCompute\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "from azureml.exceptions import ComputeTargetException\r\n",
        "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep, DatabricksStep\r\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\r\n",
        "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\r\n",
        "from azureml.train.hyperdrive import choice, loguniform\r\n",
        "from azureml.data.data_reference import DataReference\r\n",
        "from azureml.pipeline.core import PipelineParameter\r\n",
        "\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import urllib\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import mlflow\r\n",
        "import mlflow.sklearn\r\n",
        "\r\n",
        "\r\n",
        "# Check core SDK version number\r\n",
        "print(\"SDK version:\", azureml.core.VERSION)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.34.0\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1635254956737
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\r\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "distributeddeeplearningqmx\ndeep-learning-challenge\nwestus2\n3df1840f-dd4b-4f54-a831-e20536439b3a\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635254957963
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_folder = './pipeline-folder-all'\r\n",
        "# os.makedirs(script_folder, exist_ok=True)\r\n",
        "\r\n",
        "exp = Experiment(workspace=ws, name='AML_Pipeline_Comp')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635254958662
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your account info before running.\r\n",
        " \r\n",
        "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"ADBCluster\") # Databricks compute name\r\n",
        "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"<my-db-resource-group>\") # Databricks resource group\r\n",
        "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"<my-db-workspace-name>\") # Databricks workspace name\r\n",
        "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"<my-access-token>\") # Databricks access token\r\n",
        "\r\n",
        "try:\r\n",
        "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\r\n",
        "    print('Compute target {} already exists'.format(db_compute_name))\r\n",
        "except ComputeTargetException:\r\n",
        "    print('Compute not found, will use below parameters to attach new one')\r\n",
        "    print('db_compute_name {}'.format(db_compute_name))\r\n",
        "    print('db_resource_group {}'.format(db_resource_group))\r\n",
        "    print('db_workspace_name {}'.format(db_workspace_name))\r\n",
        "    print('db_access_token {}'.format(db_access_token))\r\n",
        " \r\n",
        "    config = DatabricksCompute.attach_configuration(\r\n",
        "        resource_group = db_resource_group,\r\n",
        "        workspace_name = db_workspace_name,\r\n",
        "        access_token= db_access_token)\r\n",
        "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\r\n",
        "    databricks_compute.wait_for_completion(True)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Compute target ADBCluster already exists\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635254958946
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "\r\n",
        "iris = load_iris()\r\n",
        "df = pd.DataFrame(iris['data'], columns = iris['feature_names'])\r\n",
        "df['target'] = iris['target']\r\n",
        "\r\n",
        "display(df.head())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(os.path.join(script_folder, 'datasets', 'iris.csv'), index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the default blob storage\r\n",
        "datastore = Datastore(ws, \"generalpurposeaccount\")\r\n",
        "print('Datastore {} will be used'.format(datastore.name))\r\n",
        "\r\n",
        "# We are uploading a sample file in the local directory to be used as a datasource\r\n",
        "datastore.upload_files(files=[os.path.join(script_folder, 'datasets', \"iris.csv\")], target_path=\"pipeline_inputdataset\", overwrite=False)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datastore generalpurposeaccount will be used\nUploading an estimated of 1 files\nTarget already exists. Skipping upload for pipeline_inputdataset/iris.csv\nUploaded 0 files\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 109,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_ceb87d1bd12b4d7fa834060c009e3485"
          },
          "metadata": {}
        }
      ],
      "execution_count": 109,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266403966
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from azureml.pipeline.core import PipelineParameter\r\n",
        "# pipeline_param = PipelineParameter(name=\"my_pipeline_param\", default_value=\"pipeline_param1\")\r\n",
        "\r\n",
        "step_1_input = DataReference(datastore=datastore, path_on_datastore=\"pipeline_inputdataset\",\r\n",
        "                                     data_reference_name=\"input\")\r\n",
        "\r\n",
        "step_1_processed_data = PipelineData(\"processed_data\", datastore=datastore)"
      ],
      "outputs": [],
      "execution_count": 110,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266404486
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.runconfig import EggLibrary, PyPiLibrary\r\n",
        "# egg_lib1 = EggLibrary(library=\"dbfs:/FileStore/tables/test_package_0_1_py3_8.egg\")\r\n",
        "\r\n",
        "source_directory = os.path.join(script_folder, 'databricks')\r\n",
        "python_script_name = \"data_prep.py\"\r\n",
        "\r\n",
        "dbNbStep = DatabricksStep(\r\n",
        "    name=\"DatabricksDataPrep\",\r\n",
        "    inputs=[step_1_input],\r\n",
        "    outputs=[step_1_processed_data],\r\n",
        "    source_directory=source_directory,\r\n",
        "    python_script_name=python_script_name,\r\n",
        "    python_script_params=['--input_filename', 'iris.csv', \r\n",
        "                          '--output_filename', 'iris_prep.parquet'],\r\n",
        "    existing_cluster_id=\"0908-123935-balm94\",\r\n",
        "    permit_cluster_restart=True,\r\n",
        "    run_name='DatabricksDataPrep',\r\n",
        "    compute_target=databricks_compute,\r\n",
        "    allow_reuse=False\r\n",
        ")\r\n",
        "\r\n",
        "# permit_cluster_restart=True,\r\n",
        "# notebook_name=\"/Zillow/TestNotebook\",\r\n",
        "# pypi_libraries=[PyPiLibrary('azureml-sdk[automl]')],\r\n",
        "# num_workers=1,"
      ],
      "outputs": [],
      "execution_count": 131,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266902706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\r\n",
        "\r\n",
        "dependencies:\r\n",
        "- python=3.6.2\r\n",
        "- scikit-learn\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting conda_dependencies.yml\n"
        }
      ],
      "execution_count": 132,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "\r\n",
        "# sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')\r\n",
        "sklearn_env = Environment.get(workspace=ws, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\")\r\n"
      ],
      "outputs": [],
      "execution_count": 133,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266904438
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# choose a name for your cluster\r\n",
        "cluster_name = \"hd-cluster\"\r\n",
        "\r\n",
        "try:\r\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing compute target')\r\n",
        "except ComputeTargetException:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \r\n",
        "                                                           min_nodes=0, max_nodes=4)\r\n",
        "\r\n",
        "    # create the cluster\r\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "\r\n",
        "# can poll for a minimum number of nodes and for a specific timeout. \r\n",
        "# if no min node count is provided it uses the scale settings for the cluster\r\n",
        "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\r\n",
        "\r\n",
        "# use get_status() to get a detailed status for the current cluster. \r\n",
        "print(compute_target.get_status().serialize())\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing compute target\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n{'currentNodeCount': 4, 'targetNodeCount': 4, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 4, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-10-26T15:22:33.727000+00:00', 'errors': None, 'creationTime': '2021-10-26T14:48:51.304492+00:00', 'modifiedTime': '2021-10-26T15:20:52.102477+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 4, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
        }
      ],
      "execution_count": 134,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266904877
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\r\n",
        "\r\n",
        "project_folder = os.path.join(script_folder, 'train')\r\n",
        "\r\n",
        "scriptConf = ScriptRunConfig(source_directory=project_folder,\r\n",
        "                      script='train_iris.py',\r\n",
        "                      compute_target=compute_target,\r\n",
        "                      environment=sklearn_env)\r\n"
      ],
      "outputs": [],
      "execution_count": 135,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266906431
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run = Experiment(ws, 'IrisStep').submit(scriptConf)"
      ],
      "outputs": [],
      "execution_count": 136,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266906822
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
        "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
        "\r\n",
        "param_sampling = RandomParameterSampling( {\r\n",
        "    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\r\n",
        "    \"--penalty\": choice(0.5, 1, 1.5)\r\n",
        "    }\r\n",
        ")\r\n",
        "\r\n",
        "hyperdrive_config = HyperDriveConfig(run_config=scriptConf,\r\n",
        "                                     hyperparameter_sampling=param_sampling, \r\n",
        "                                     primary_metric_name='Accuracy',\r\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\r\n",
        "                                     max_total_runs=4,\r\n",
        "                                     max_concurrent_runs=4)\r\n"
      ],
      "outputs": [],
      "execution_count": 137,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266907277
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_output_name = 'metrics_output'\r\n",
        "metrics_data = PipelineData(name='metrics_data',\r\n",
        "                            datastore=datastore,\r\n",
        "                            pipeline_output_name=metrics_output_name,\r\n",
        "                            training_output=TrainingOutput(\"Metrics\"))\r\n",
        "\r\n",
        "model_output_name = 'model_output'\r\n",
        "saved_model = PipelineData(name='saved_model',\r\n",
        "                            datastore=datastore,\r\n",
        "                            pipeline_output_name=model_output_name,\r\n",
        "                            training_output=TrainingOutput(\"Model\",\r\n",
        "                                                           model_file=\"outputs/model.joblib\"))\r\n",
        "\r\n",
        "hd_step_name='hd_step01'\r\n",
        "hd_step = HyperDriveStep(\r\n",
        "    name=hd_step_name,\r\n",
        "    hyperdrive_config=hyperdrive_config,\r\n",
        "    estimator_entry_script_arguments=['--input_data', step_1_processed_data, '--input_filename', 'iris_prep.parquet'],\r\n",
        "    inputs=[step_1_processed_data],\r\n",
        "    allow_reuse=False,\r\n",
        "    outputs=[metrics_data, saved_model])"
      ],
      "outputs": [],
      "execution_count": 138,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266908108
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conda_dep = CondaDependencies()\r\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")\r\n",
        "\r\n",
        "rcfg = RunConfiguration(conda_dependencies=conda_dep)\r\n",
        "\r\n",
        "single_node_compute_target = ComputeTarget(workspace=ws, name='singlenode')\r\n",
        "\r\n",
        "source_directory_m_reg = os.path.join(script_folder, 'register')\r\n",
        "python_script_name = \"eval_register_model.py\"\r\n",
        "\r\n",
        "register_model_step = PythonScriptStep(source_directory=source_directory_m_reg,\r\n",
        "                                       script_name=python_script_name,\r\n",
        "                                       name=\"register_model_step\",\r\n",
        "                                       inputs=[saved_model, metrics_data],\r\n",
        "                                       compute_target=single_node_compute_target,\r\n",
        "                                       arguments=[\"--saved-model\", saved_model, '--metrics', metrics_data, '--model_name', 'iris_model_pipeline'],\r\n",
        "                                       allow_reuse=True,\r\n",
        "                                       runconfig=rcfg)\r\n",
        "\r\n",
        "# register_model_step.run_after(hd_step)"
      ],
      "outputs": [],
      "execution_count": 139,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635266908784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run = Experiment(ws, 'IrisStep').submit(scriptConf)\r\n",
        "steps = [dbNbStep, hd_step, register_model_step]\r\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\r\n",
        "run = exp.submit(pipeline)\r\n",
        "# pipeline_run = exp.submit(pipeline)\r\n",
        "# pipeline_run.wait_for_completion()\r\n",
        "run"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step DatabricksDataPrep [800e0dda][96c016b0-0a3b-4306-9cf9-7bb6d4a7f13b], (This step will run and generate new outputs)\nCreated step hd_step01 [80c0f9ca][3a6141dc-6078-47a0-810c-7586dc8112d0], (This step will run and generate new outputs)\nCreated step register_model_step [2d82b646][249aa4b8-5cf9-46f1-b6a2-4f3522afbc35], (This step is eligible to reuse a previous run's output)\nUsing data reference input for StepId [f059ac3f][60e2688b-3e36-47d8-abab-eb677c1b6329], (Consumers of this data are eligible to reuse prior runs.)\nSubmitted PipelineRun f4175e6f-85a9-4bb3-8b36-7928c83f94ac\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/f4175e6f-85a9-4bb3-8b36-7928c83f94ac?wsid=/subscriptions/3df1840f-dd4b-4f54-a831-e20536439b3a/resourcegroups/deep-learning-challenge/workspaces/distributeddeeplearningqmx&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 141,
          "data": {
            "text/plain": "Run(Experiment: AML_Pipeline_Comp,\nId: f4175e6f-85a9-4bb3-8b36-7928c83f94ac,\nType: azureml.PipelineRun,\nStatus: Preparing)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>AML_Pipeline_Comp</td><td>f4175e6f-85a9-4bb3-8b36-7928c83f94ac</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/f4175e6f-85a9-4bb3-8b36-7928c83f94ac?wsid=/subscriptions/3df1840f-dd4b-4f54-a831-e20536439b3a/resourcegroups/deep-learning-challenge/workspaces/distributeddeeplearningqmx&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 141,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635267762285
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineEndpoint\r\n",
        "p_endpoint = PipelineEndpoint.publish(workspace=ws, name=\"prep_train_reg\", pipeline=pipeline, description=\"\")"
      ],
      "outputs": [],
      "execution_count": 146,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635268277508
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}